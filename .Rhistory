v.names = "conc", direction = "long") %>% View()
?aggregate
aggregate(state.x77, list(Region = state.region), mean)
aggregate(state.x77, list(Region = state.region), mean) %>% View()
View(state.x77)
str(state.x77)
state.region
aggregate(state.x77,
list(Region = state.region,
Cold = state.x77[,"Frost"] > 130),
mean)
aggregate(state.x77,
list(Region = state.region,
Cold = state.x77[,"Frost"] > 130),
mean) %>% View()
testDF <- data.frame(v1 = c(1,3,5,7,8,3,5,NA,4,5,7,9),
v2 = c(11,33,55,77,88,33,55,NA,44,55,77,99) )
by1 <- c("red", "blue", 1, 2, NA, "big", 1, 2, "red", 1, NA, 12)
by2 <- c("wet", "dry", 99, 95, NA, "damp", 95, 99, "red", 99, NA, NA)
aggregate(x = testDF, by = list(by1, by2), FUN = "mean") %>% View()
View(testDF)
aggregate(x = testDF, by = list(by1, by2), FUN = "mean") %>% View()
?xtabs
ag <- aggregate(len ~ ., data = ToothGrowth, mean) %>% View()
xtabs(len ~ ., data = ag) %>% View()
ag <- aggregate(len ~ ., data = ToothGrowth, mean) %>% View()
xtabs(len ~ ., data = ag) %>% View()
ag <- aggregate(len ~ ., data = ToothGrowth, mean)
xtabs(len ~ ., data = ag) %>% View()
ag <- aggregate(len ~ ., data = ToothGrowth, mean)
View(ag)
xtabs(len ~ ., data = ag) %>% View()
View(ToothGrowth)
View(ag)
aggregate(. ~ Species, data = iris, mean) %>% View()
View(iris)
library(tidyverse)
stocks <- tibble(
year   = c(2015, 2015, 2016, 2016),
half  = c(   1,    2,     1,    2),
return = c(1.88, 0.59, 0.92, 0.17)
)
stocks
stocks %>% spread(key = half, value = return)
stocks %>% spread(key = half, value = return) %>% gather()
stocks %>% spread(key = half, value = return) %>% gather('1':'2', key = year, value = cases)
stocks %>% spread(key = half, value = return) %>% gather('1':'2', key = half, value = cases)
stocks %>% spread(key = half, value = return)
?spread
table4a = tibble(country = c())
#> # A tibble: 3 × 3
#>       country `1999` `2000`
#> *       <chr>  <int>  <int>
#> 1 Afghanistan    745   2666
#> 2      Brazil  37737  80488
#> 3       China 212258 213766
table4a = tibble(country = c("Afghanistan", "Brazil","China"))
#> # A tibble: 3 × 3
#>       country `1999` `2000`
#> *       <chr>  <int>  <int>
#> 1 Afghanistan    745   2666
#> 2      Brazil  37737  80488
#> 3       China 212258 213766
table4a = tibble(country = c("Afghanistan", "Brazil","China"))
table4a
table4a['1999'] = c(745, 37737, 212258)
table4a['2000'] = c(2666,80488, 213766)
table4a
table4a %>%
gather(1999, 2000, key = "year", value = "cases")
?gather
table4a %>%
gather('1999', '2000', key = "year", value = "cases")
table4a %>%
gather(2, 3, key = "year", value = "cases")
table4a %>%
gather(1, 2, key = "year", value = "cases")
people <- tribble(
~name,             ~key,    ~value,
#-----------------|--------|------
"Phillip Woods",   "age",       45,
"Phillip Woods",   "height",   186,
"Phillip Woods",   "age",       50,
"Jessica Cordero", "age",       37,
"Jessica Cordero", "height",   156
)
people
people %>% spread(key = key, value = value )
?spread
?sample
sample(1:100000,100000 )
?write()
?read.csv
?scan
library(parallel)
parallel::detectCores()
devtools::install_github("lch14forever/microbiomeViz")
install.packages("devtools")
devtools::install_github("lch14forever/microbiomeViz")
install.packages("git2r")
sys.call("gcc")
sys.call("gcc --version")
system("gcc")
system("gcc --version")
install.packages("git2r")
install.packages("git2r")
install.packages("git2r")
?solve
example("solve")
hilbert <- function(n) { i <- 1:n; 1 / outer(i - 1, i, "+") }
h8 <- hilbert(8); h8
h8
h8
solve(h8)
h8sh8 <- solve(h8)
sh8 <- solve(h8)
round(sh8 %*% h8, 3)
h8
solve(t(h8) %*% h8)
sample(10, 10)
?sample
sample(10, 16)
sample(1:20, 16)
matrix(sample(1:20, 16), nrow = 4)
a = matrix(sample(1:20, 16), nrow = 4)
solve(a)
a = matrix(c(-5,-3,2,1), nrow = 2)
a
a = matrix(c(-5,-3,2,1), nrow = 2, byrow = TRUE)
a
solve(a)
solve(t(a) %*% a)
solve(t(a) %*% a) %*% t(a)
x1 = 1:10
x2 = 11:20
length(1x)
length(x1)
length(x2)
x1 * x2
if(1 == 1) for (i in 1:10) print(i)
if(i == 1) for (i in 1:10) print(i)
i
j
if(j == 1) for (j in 1:10) print(i)
?apply
fcdfkdsaf
shiny::runGitHub("milineage","2533245542")
shiny::runGitHub("shiny-phyloseq","joey711")
library(readr)
time <- read_delim("~/Desktop/python_analysis/vary_python_more/time.csv",
" ", escape_double = FALSE, col_names = FALSE,
trim_ws = TRUE)
View(time)
library(readr)
time2 <- read_delim("~/Desktop/python_analysis/vary_python_more/time2.csv",
" ", escape_double = FALSE, col_names = FALSE,
trim_ws = TRUE)
View(time2)
library(readr)
time3 <- read_delim("~/Desktop/python_analysis/vary_python_more/time3.csv",
" ", escape_double = FALSE, col_names = FALSE,
trim_ws = TRUE)
View(time3)
qplot(x = time3$X1, y = time3$X2, geom = 'line')
time3 = time3[-1,]
View(time3)
qplot(x = time3$X1, y = time3$X2, geom = 'line')
time3 = time3[time3$X1 < 50,]
qplot(x = time3$X1, y = time3$X2, geom = 'line')
qplot(x = time3$X1, y = time3$X2, geom = 'line')
View(time3)
library(readr)
time3 <- read_delim("~/Desktop/python_analysis/vary_python_more/time3.csv",
" ", escape_double = FALSE, col_names = FALSE,
trim_ws = TRUE)
View(time3)
time3 = time3[time3$X2 < 50,]
View(time3)
qplot(x = time3$X1, y = time3$X2, geom = 'line')
qplot(x = time2$X1, y = time2$X2, geom = 'line')
qplot(x = time$X1, y = time$X2, geom = 'line')
time$X3 = 'first'
time2$X3 = 'second'
time3$X3 = 'third'
rbind(time, time2, time3)
all = rbind(time, time2, time3)
qplot(x = X1, y = X2, data = all, colour = X3)
qplot(x = X1, y = X2, data = all, colour = X3, geom = 'line')
qplot(x = X1, y = X2, data = all, colour = X3, geom = 'smooth')
all$X1 = all$X1 /1000
qplot(x = X1, y = X2, data = all, colour = X3, geom = 'smooth')
qplot(x = X1, y = X2, data = all, colour = X3, geom = 'smooth', x = 'read buffer size(kb)', time takes to finish(s), main = 'The time it takes for python to read 1GB data\n flutuates with read buffer size\n and reaches the minimum at 100 kb read buffer size') + guides(fill=guide_legend(title="New Legend Title"))
qplot(x = X1, y = X2, data = all, colour = X3, geom = 'smooth', x = 'read buffer size(kb)', y= 'time takes to finish(s)', main = 'The time it takes for python to read 1GB data\n flutuates with read buffer size\n and reaches the minimum at 100 kb read buffer size') + guides(fill=guide_legend(title="New Legend Title"))
qplot(x = X1, y = X2, data = all, colour = X3, geom = 'smooth', xlab = 'read buffer size(kb)', ylab= 'time takes to finish(s)', main = 'The time it takes for python to read 1GB data\n flutuates with read buffer size\n and reaches the minimum at 100 kb read buffer size') + guides(fill=guide_legend(title="New Legend Title"))
qplot(x = X1, y = X2, data = all, colour = X3, geom = 'smooth', xlab = 'read buffer size(kb)', ylab= 'time takes to finish(s)', main = 'The time it takes for python to read 1GB data\n flutuates with read buffer size\n and reaches the minimum at 100 kb read buffer size')
qplot(x = X1, y = X2, data = all, colour = X3, geom = 'smooth', xlab = 'read buffer size(kb)', ylab= 'time takes to finish(s)', main = 'The time it takes for python to read 1GB data\n flutuates with read buffer size\n and reaches the minimum at 100 kb read buffer size') +scale_fill_discrete(name = "New Legend Title")
qplot(x = X1, y = X2, data = all, colour = X3, geom = 'smooth', xlab = 'read buffer size(kb)', ylab= 'time takes to finish(s)', main = 'The time it takes for python to read 1GB data\n flutuates with read buffer size\n and reaches the minimum at 100 kb read buffer size') + labs(color = "sale year")
qplot(x = X1, y = X2, data = all, colour = X3, geom = 'smooth', xlab = 'read buffer size(kb)', ylab= 'time takes to finish(s)', main = 'The time it takes for python to read 1GB data\n flutuates with read buffer size\n and reaches the minimum at 100 kb read buffer size') + labs(color = "trials")
qplot(x = X1, y = X2, data = all, colour = X3, geom = 'line', xlab = 'read buffer size(kb)', ylab= 'time takes to finish(s)', main = 'The time it takes for python to read 1GB data\n flutuates with read buffer size\n and reaches the minimum at 100 kb read buffer size') + labs(color = "trials")
p <- ggplot(mtcars, aes(wt, mpg))
library(ggplot2)
p <- ggplot(mtcars, aes(wt, mpg))
p + geom_point(aes(alpha = 0.2))
p + geom_point(aes(alpha = 1))
p + geom_point()
p + geom_point(aes(alpha = 20))
p + geom_point(aes(alpha = 0.001))
p + geom_point(aes(alpha = 0.0002))
p + geom_point(alpah = 1)
p + geom_point(alpha = 1)
p + geom_point(alpha = 2)
p + geom_point(alpha = 0.01)
p + geom_point(alpha = 0.2)
p + geom_point(size = 0.2)
p + geom_point(aes(size = 0.2))
myalpha = 0.2
p + geom_point(alpah = myalpah)
p + geom_point(alpah = myalpha)
p + geom_point(alphja = myalpha)
p + geom_point(alpha = myalpha)
scan("4854
4833
2098
2672
1839
4836
5156", what = numeric())
a = c(4854,
4833,
2098,
2672,
1839,
4836,
5156)
a1 = a
cs_code = a
bio_code = c(4854,
4854,
4854,
4854,
3987,
4837,
2619,
1839,
5816,
5816,
3514,
1846,
1846,
3451,
4704)
intersect(cs_code, bio_code)
union(cs_code, bio_code)
load("~/Downloads/filtered_phyloseq_object-4.RData")
tax = tax_table(filtered_phyloseq_obj)
otu = otu_table(filtered_phyloseq_obj)
tax
library("shiny")
shiny::runGitHub("miStudio","2533245542")
runGitHub("miStudio","2533245542")
# 3.13
# part1
a = c(3129, 3000, 2865, 2890, 3200, 3300, 2975, 3150, 2800, 2900, 2985, 3050, 2600, 2700, 2600, 2765)
a1 = matrix(a, nrow = 4, byrow = TRUE)
a2 = as.data.frame(a1)
a2
# 3.13
# part1
a = c(3129, 3000, 2865, 2890, 3200, 3300, 2975, 3150, 2800, 2900, 2985, 3050, 2600, 2700, 2600, 2765)
a1 = matrix(a, nrow = 4, byrow = TRUE)
a2 = as.data.frame(a1)
a3 = c(mean(a2$V1), mean(a2$V2), mean(a2$V3), mean(a2$V4))
a4 = as.data.frame(a3)
a4
a3 = as.data.frame(mean(a2$V1), mean(a2$V2), mean(a2$V3), mean(a2$V4))
a3
a3 = as.data.frame(v1 = mean(a2$V1), v2 = mean(a2$V2), v3 = mean(a2$V3), v4 = mean(a2$V4))
a3
a3 = as.data.frame(list(mean(a2$V1), mean(a2$V2), mean(a2$V3), mean(a2$V4)))
a3
t.test(a3[[1]], a3[[2]], var.equal = TRUE, alternative="two.sided", conf.level=0.95) #mean has no varaice
?t.test()
t.test(a2$V1~a2$V2,conf.level=0.95) #mean has no varaice
t.test(a2$V1,a2$V2,conf.level=0.95) #mean has no varaice
t.test(a2$V3,a2$V1,a2$V2,conf.level=0.95) #mean has no varaice
t.test(a2$V3, a2$V1,a2$V2,conf.level=0.95) #mean has no varaice
t.test(a2$V1conf.level=0.95) #mean has no varaice
t.test(a2$V1,conf.level=0.95) #mean has no varaice
t.test(a2$V1,conf.level=0.95)
t.test(a2$V2,conf.level=0.95)
t.test(a2$V3,conf.level=0.95)
t.test(a2$V4,conf.level=0.95)
t.test(a2$V1,a2$V3,conf.level=0.95)
a = c(3129, 3000, 2865, 2890, 3200, 3300, 2975, 3150, 2800, 2900, 2985, 3050, 2600, 2700, 2600, 2765)
a1 = matrix(a, nrow = 4, byrow = FALSE)
a2 = as.data.frame(a1)
t.test(a2$V1,conf.level=0.95)
t.test(a2$V2,conf.level=0.95)
t.test(a2$V3,conf.level=0.95)
t.test(a2$V4,conf.level=0.95)
t.test(a2$V1,a2$V3,conf.level=0.95)
s1sq <- var(a)
s2sq <- var(b)
s1sq/s2sq
a = c(83,85,85,87,90,88,88,84,91,90,91, 87, 84, 87, 85, 86, 83)
b = c(91, 87, 84, 87, 85, 86, 83)
s1sq <- var(a)
s2sq <- var(b)
s1sq/s2sq
c(qf(0.05/2, 16, 7), qf(1-0.05/2, 16, 7))
t.test(a,b, conf.level = 0.95, var.equal = TRUE)
s1sq <- var(a2$V1)
s2sq <- var(a2$V3)
s1sq/s2sq
c(qf(0.05/2, 3, 3), qf(1-0.05/2, 3, 3)) # assume equal variance
t.test(a2$V1,a2$V3,conf.level=0.95, var.equal = TRUE)
knitr::opts_chunk$set(echo = TRUE)
type1 = c(65,81,57,66,82,82,67,59,75,70)
knitr::opts_chunk$set(echo = TRUE)
type2 = c(64,71,83,59,65,56,69,74,82,79)
type1 = c(65,81,57,66,82,82,67,59,75,70)
data.frame(type1, type2)
# 2.28
type1 = c(65,81,57,66,82,82,67,59,75,70)
type2 = c(64,71,83,59,65,56,69,74,82,79)
data = data.frame(type1, type2)
var.test(data$type1,data$type2,alternative="two.sided",conf.level = 0.95)
t.test(data$type1,data$type2, alternative = "two.sided", conf.level = 0.95)
# 2.28
# a),b)
type1 = c(65,81,57,66,82,82,67,59,75,70)
type2 = c(64,71,83,59,65,56,69,74,82,79)
data = data.frame(type1, type2)
var.test(data$type1,data$type2,alternative="two.sided",conf.level = 0.95)
t.test(data$type1,data$type2, alternative = "two.sided", conf.level = 0.95)
# c)
X_std <- scale(data$type1)
qqnorm(X_std)
abline(a=0, b=1, col="red",lwd=2)
Y_std <- scale(data$type2)
qqnorm(Y_std)
abline(a=0,b=1,col="red",lwd=2)
# the data are one the line so they are normal
install.packages(ggpubr)
install.packages("ggpubr")
a = c(3129, 3000, 2865, 2890, 3200, 3300, 2975, 3150, 2800, 2900, 2985, 3050, 2600, 2700, 2600, 2765)
a1 = matrix(a, nrow = 4, byrow = FALSE)
a2 = as.data.frame(a1)
a2
a = c(3129, 3000, 2865, 2890, 3200, 3300, 2975, 3150, 2800, 2900, 2985, 3050, 2600, 2700, 2600, 2765)
a2 = c(1,2,3,4)
a3 = list(a, a2)
a3
dotplot(a3)
dotchart(a3)
dotchart(a3[[1]])
library(ggplot2)
library(ggpubr)
## 2.28
# a),b)
type1 = c(65,81,57,66,82,82,67,59,75,70)
type2 = c(64,71,83,59,65,56,69,74,82,79)
data = data.frame(type1, type2)
var.test(data$type1,data$type2,alternative="two.sided",conf.level = 0.95)
t.test(data$type1,data$type2, alternative = "two.sided", conf.level = 0.95)
# c)
X_std <- scale(data$type1)
qqnorm(X_std)
abline(a=0, b=1, col="red",lwd=2)
Y_std <- scale(data$type2)
qqnorm(Y_std)
abline(a=0,b=1,col="red",lwd=2)
# the data are one the line so they are normal
## 2.30
## 2.34
## 2.36
karl = c(1.186, 1.151, 1.322, 1.339, 1.200, 1.402, 1.365, 1.537, 1.559)
leh = c(1.061, 0.992, 1.063, 1.062, 1.065, 1.178, 1.037, 1.086, 1.052)
diff = karl - leh
data = data.frame(karl, leh, diff)
t.test(data$karl, data$leh, alternative = "two.sided", conf.level = 0.95)
#p value = 0.0003557  is very small, reject null hypothesis. Support the claim that there is a difference in mean performance.
ggqqplot(data$karl) # for karlsuhe
ggqqplot(data$leh) # for lehigh
ggqqplot(data$diff) # for difference
# 95 % CI is (0.1590886, 0.3886892)
# d)
qqnorm(data$karl)
abline(a = 0, b = 1, col = 'red', lwd = 2)
# the dots are not aligned with the line, so not normal
qqnorm(data$leh)
abline(a = 0, b = 1, col = 'red', lwd = 2)
# the dots are not aligned with the line, so not normal
# e)
qqnorm(data$leh)
abline(a = 0, b = 1, col = 'red', lwd = 2)
# the dots are not aligned with the line, so not normal
# 3.7
# 3.11
# part1
a = c(3129, 3000, 2865, 2890, 3200, 3300, 2975, 3150, 2800, 2900, 2985, 3050, 2600, 2700, 2600, 2765)
a2 = c(1,2,3,4)
a3 = list(a, a2)
aov_c = aov(a3[[1]]~a3[[2]])
aov
?aov
library(ggplot2)
library(ggpubr)
## 2.28
# a),b)
type1 = c(65,81,57,66,82,82,67,59,75,70)
type2 = c(64,71,83,59,65,56,69,74,82,79)
data = data.frame(type1, type2)
var.test(data$type1,data$type2,alternative="two.sided",conf.level = 0.95)
t.test(data$type1,data$type2, alternative = "two.sided", conf.level = 0.95)
# c)
X_std <- scale(data$type1)
qqnorm(X_std)
abline(a=0, b=1, col="red",lwd=2)
Y_std <- scale(data$type2)
qqnorm(Y_std)
abline(a=0,b=1,col="red",lwd=2)
# the data are one the line so they are normal
## 2.30
## 2.34
## 2.36
karl = c(1.186, 1.151, 1.322, 1.339, 1.200, 1.402, 1.365, 1.537, 1.559)
leh = c(1.061, 0.992, 1.063, 1.062, 1.065, 1.178, 1.037, 1.086, 1.052)
diff = karl - leh
data = data.frame(karl, leh, diff)
t.test(data$karl, data$leh, alternative = "two.sided", conf.level = 0.95)
#p value = 0.0003557  is very small, reject null hypothesis. Support the claim that there is a difference in mean performance.
ggqqplot(data$karl) # for karlsuhe
ggqqplot(data$leh) # for lehigh
ggqqplot(data$diff) # for difference
# 95 % CI is (0.1590886, 0.3886892)
# d)
qqnorm(data$karl)
abline(a = 0, b = 1, col = 'red', lwd = 2)
# the dots are not aligned with the line, so not normal
qqnorm(data$leh)
abline(a = 0, b = 1, col = 'red', lwd = 2)
# the dots are not aligned with the line, so not normal
# e)
qqnorm(data$leh)
abline(a = 0, b = 1, col = 'red', lwd = 2)
# the dots are not aligned with the line, so not normal
# 3.7
# 3.11
# part1
a = c(3129, 3000, 2865, 2890, 3200, 3300, 2975, 3150, 2800, 2900, 2985, 3050, 2600, 2700, 2600, 2765)
a1 = matrix(a, nrow = 4, byrow = FALSE)
a2 = as.data.frame(a1)
t.test(a2$V1,a2$V3,conf.level=0.95)
# p value smaller than 0.05, we know mixing techniques affect strength of cement.
dotchart(a3[[1]]) # dot plot
# 3.13
# part1
a = c(3129, 3000, 2865, 2890, 3200, 3300, 2975, 3150, 2800, 2900, 2985, 3050, 2600, 2700, 2600, 2765)
a1 = matrix(a, nrow = 4, byrow = FALSE)
a2 = as.data.frame(a1)
t.test(a2$V1,conf.level=0.95)
t.test(a2$V2,conf.level=0.95)
t.test(a2$V3,conf.level=0.95)
t.test(a2$V4,conf.level=0.95)
# F test
s1sq <- var(a2$V1)
s2sq <- var(a2$V3)
s1sq/s2sq
c(qf(0.05/2, 3, 3), qf(1-0.05/2, 3, 3)) # assume equal variance
# t-test
t.test(a2$V1,a2$V3,conf.level=0.95, var.equal = TRUE)
# It helps a lot for interpreting the result. The means does not tell much difference between the
# two techniques but a T-test tells us that there is a big difference between technique 1 and 3.
# 3.14
# 3.17
a1 = c(3, 5, 3, 7, 6, 5, 3, 2, 1, 6)
a2 = c(1,3,4,7,5,6,3,2,1,7)
a3 = c(4,1,3,5,7,1,2,4,2,7)
a4 = c(3, 5, 7, 5, 10, 3, 4, 7, 2, 7)
aa = data.frame(a1,a2,a3,a4)
# 3.18
getwd()
5*5*64 + 5*5*192 + 3*3*384 + 5*5*256 + 5*5*256 + 4096*256 + 4096*4096 + 4096*10
5*5*64 + 2*64 + 5*5*192 + 2*192 + 3*3*384 + 2*384 + 5*5*256 + 2*256 + 5*5*256 + 2*256 + 4096 + 4096*2 + 4096 + 4096*2 + 10
5*5*64 + 2*64 +5*5*192 + 2*192+3*3*384 + 2*384+5*5*256 + 2*256+5*5*256 + 2*256+4096*256 + 4096*2+4096*4096 + 4096*2+4096*10
(3*5*5 +1)*64 + (64*5*5+1)*192+ (192*3*3+1)*384+ (384*5*5+1)*256 + (256*5*5+1)*256 + (4096+1)*256 + (4096+1)*4096 + (4096+1)*10
(3*5*5 +1)*64
(64*5*5+1)*192
(192*3*3+1)*384
(384*5*5+1)*256
(256*5*5+1)*256
(4096+1)*256
(4096+1)*4096
(4096+1)*10
(3*5*5 +1)*64 + 2*264
(64*5*5+1)*192 + *22192
(64*5*5+1)*192 + 2*2192
(192*3*3+1)*384 + 2*2384
(384*5*5+1)*256 + 2*2256
(256*5*5+1)*256 + 2*2256
(4096+1)*256 + 4096*2
(4096+1)*4096 + 4096*2
(4096+1)*10
(3*5*5 +1)*64 + 2*264 +(64*5*5+1)*192 + 2*2192 +(192*3*3+1)*384 + 2*2384 + (384*5*5+1)*256 + 2*2256 + (256*5*5+1)*256 + 2*2256 + (4096+1)*256 + 4096*2 + (4096+1)*4096 + 4096*2 + (4096+1)*10
(3*5*5 +1)*64 + 2*264 + (64*5*5+1)*192 + 2*2192 + (192*3*3+1)*384 + 2*2384 +(384*5*5+1)*256 + 2*2256 +(256*5*5+1)*256 + 2*2256 +(256 + 1)*4096 + 4096*2 + (4096 + 1)*4096 + 4096*2 + (4096 + 1)*10
(256 + 1)*4096 + 4096*2
(4096 + 1)*4096 + 4096*2
(4096 + 1)*10
setwd("~/Desktop/miStudio")
library(shiny)
runApp(appDir = )
